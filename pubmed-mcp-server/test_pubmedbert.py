#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PubMedBERT –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import torch
from transformers import AutoTokenizer, AutoModel, pipeline
import numpy as np

class PubMedBERTAnalyzer:
    def __init__(self):
        self.model_name = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
        self.tokenizer = None
        self.model = None
        self.classifier = None
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç PubMedBERT"""
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º PubMedBERT...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            print("‚úÖ PubMedBERT –∑–∞–≥—Ä—É–∂–µ–Ω!")
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π/–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            self.classifier = pipeline(
                "text-classification",
                model="microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
                tokenizer="microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
            )
            print("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
        
        return True
    
    def get_embeddings(self, text):
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.model:
            return None
            
        inputs = self.tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # –ë–µ—Ä–µ–º [CLS] —Ç–æ–∫–µ–Ω –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            embeddings = outputs.last_hidden_state[:, 0, :].numpy()
            
        return embeddings[0]
    
    def analyze_safety(self, supplement_text):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–∫–∏"""
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {supplement_text}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        safety_contexts = [
            f"Safety and side effects of {supplement_text}",
            f"Adverse reactions and contraindications of {supplement_text}",
            f"Drug interactions with {supplement_text}",
            f"Dosage and toxicity of {supplement_text}",
            f"Clinical studies on {supplement_text} safety"
        ]
        
        results = []
        
        for context in safety_contexts:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                embeddings = self.get_embeddings(context)
                
                if embeddings is not None:
                    # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    safety_score = float(np.mean(embeddings))
                    
                    results.append({
                        "context": context,
                        "safety_score": safety_score,
                        "embeddings_shape": embeddings.shape
                    })
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ '{context}': {e}")
        
        return results
    
    def compare_supplements(self, supplement1, supplement2):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞"""
        print(f"‚öñÔ∏è –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º: {supplement1} vs {supplement2}")
        
        emb1 = self.get_embeddings(f"Medical properties and effects of {supplement1}")
        emb2 = self.get_embeddings(f"Medical properties and effects of {supplement2}")
        
        if emb1 is not None and emb2 is not None:
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            
            return {
                "supplement1": supplement1,
                "supplement2": supplement2,
                "similarity": float(similarity),
                "interpretation": "–í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ" if similarity > 0.8 else "–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ" if similarity > 0.6 else "–ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"
            }
        
        return None

def test_pubmedbert():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç PubMedBERT"""
    
    analyzer = PubMedBERTAnalyzer()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if not analyzer.load_model():
        return
    
    print("\n" + "="*60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï PubMedBERT")
    print("="*60)
    
    # –¢–µ—Å—Ç 1: –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–∞–≥–Ω–∏—è
    print("\n1Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–∞–≥–Ω–∏—è:")
    magnesium_results = analyzer.analyze_safety("magnesium supplement")
    
    for result in magnesium_results[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
        print(f"   üìä {result['context']}")
        print(f"   üî¢ –û—Ü–µ–Ω–∫–∞: {result['safety_score']:.4f}")
        print()
    
    # –¢–µ—Å—Ç 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–±–∞–≤–æ–∫
    print("\n2Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–±–∞–≤–æ–∫:")
    comparison = analyzer.compare_supplements("magnesium", "calcium")
    if comparison:
        print(f"   ‚öñÔ∏è {comparison['supplement1']} vs {comparison['supplement2']}")
        print(f"   üìà –°—Ö–æ–¥—Å—Ç–≤–æ: {comparison['similarity']:.4f}")
        print(f"   üí≠ {comparison['interpretation']}")
    
    # –¢–µ—Å—Ç 3: –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–±–∞–≤–æ–∫
    print("\n3Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–±–∞–≤–æ–∫:")
    supplements = ["vitamin D", "omega-3", "turmeric", "green tea extract"]
    
    for supplement in supplements:
        print(f"\n   üîç {supplement.title()}:")
        results = analyzer.analyze_safety(supplement)
        if results:
            avg_score = np.mean([r['safety_score'] for r in results])
            print(f"   üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.4f}")

if __name__ == "__main__":
    test_pubmedbert()